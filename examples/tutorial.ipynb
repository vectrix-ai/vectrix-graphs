{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectrix Graphs\n",
    "\n",
    "### Packages needed to run this notebook\n",
    "- ***libmagic***: ```brew install libmagic```\n",
    "*This is used to extract the file type from the file.*\n",
    "\n",
    "\n",
    "### ChromaDB\n",
    "For lightweight usage, don't forget to start the ChromaDB server: ```docker run -p 7777:8000 chromadb/chroma```\n",
    "\n",
    "### Weaviate\n",
    "If you want to use Weaviate instead of ChromaDB (which I do recommand for stability and performance), don't forget to start the Weaviate server: ```docker-compose -p weaviate up -d```\n",
    "\n",
    "It's important to run this command from the ```src/vectrix_graphs/db``` directory. You also need a .env file in this directory with the following content:\n",
    "```\n",
    "COHERE_API_KEY=my_cohere_api_key\n",
    "```\n",
    "\n",
    "### Run the API\n",
    "https://docs.astral.sh/uv/guides/integration/fastapi/\n",
    "\n",
    "For development purposes, you can run the API with: ```uv run fastapi dev```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m2024-11-08 14:22:48,490 - VectorDB - WARNING - Demo collection already exists\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load packages from the src directory\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('../src')\n",
    "from vectrix_graphs import ExtractDocuments, setup_logger, ExtractMetaData\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting chunks of data from a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chunks of data from a document\n",
    "extract = ExtractDocuments(\n",
    "    logger=setup_logger(name=\"Files\", level=\"INFO\"),\n",
    "    )\n",
    "\n",
    "result = extract.extract(file_path=\"./files/attention_is_all_you_need.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metadata:')\n",
    "print(json.dumps(result[1].metadata, indent=4))\n",
    "\n",
    "print('Content:')\n",
    "print(result[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional metadata using a NER-pipeline\n",
    "ner = ExtractMetaData(\n",
    "    logger=setup_logger(name=\"ExtractMetaData\", level=\"INFO\",),\n",
    "    model=\"gpt-4o-mini\" # Options are gpt-4o-mini, llama3.1-8B , llama3.1-70B\n",
    "    )\n",
    "result_with_metadata = ner.extract(result, source=\"uploaded_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_metadata[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the documents to a vector database (Chroma)\n",
    "\n",
    "For this demo, the vector database will be saved locally on disk, restarting the container will delete the database.\n",
    "I prefer using the cosine distance instead of the default squared L2 distance, we pass this using the `hnsw:space` metadata.\n",
    "\n",
    "$$\n",
    "d = 1.0 - \\frac{\\sum(A_i \\times B_i)}{\\sqrt{\\sum(A_i^2) \\cdot \\sum(B_i^2)}}\n",
    "$$\n",
    "\n",
    "We use Ollama to calculate the embeddings locally with BGE-M3, since over a 100 langues are supported this is ideal for embedding Arabic documents.\n",
    "\n",
    "BGE-M3 is based on the XLM-RoBERTa architecture and is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity:\n",
    "\n",
    "- Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval.\n",
    "- Multi-Linguality: It can support more than 100 working languages.\n",
    "- Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens.\n",
    "\n",
    "> ℹ️ So all embeddings will be calculated locally ℹ️\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectrix_graphs import vectordb\n",
    "\n",
    "vectordb.remove_collection(\"demo\")\n",
    "vectordb.create_collection(\"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.add_documents(result_with_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'language': 'EN', 'tags': \"['AI', 'Machine Learning', 'Natural Language Processing']\", 'read_time': 0.685, 'summary': 'The document discusses self-attention mechanisms, particularly in the context of the Transformer model, highlighting its advantages over traditional models that use RNNs or convolution. It also mentions applications of self-attention in various tasks such as reading comprehension and language modeling.', 'source': 'uploaded_file', 'filetype': 'application/pdf', 'last_modified': '2024-10-22T21:44:24', 'content_type': 'blog_post', 'filename': 'attention_is_all_you_need.pdf', 'word_count': 137.0, 'author': '', 'uuid': '98a952ae-df5b-4bf7-93f8-ee9d2ada043d'}, page_content='Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].'),\n",
       " Document(metadata={'language': 'EN', 'tags': \"['AI', 'Attention Mechanism', 'Natural Language Processing']\", 'read_time': 0.565, 'source': 'uploaded_file', 'summary': \"The content discusses the attention mechanism in neural networks, specifically focusing on how attention heads can capture long-distance dependencies in language processing, using the verb 'making' as an example.\", 'filetype': 'application/pdf', 'last_modified': '2024-10-22T21:44:24', 'content_type': 'blog_post', 'filename': 'attention_is_all_you_need.pdf', 'word_count': 113.0, 'author': '', 'uuid': '950bcbfb-6a01-4c8b-b414-0679026c3d52'}, page_content='American\\n\\nthis\\n\\nmaking\\n\\n<pad>\\n\\n<pad>\\n\\nthe\\n\\n<pad>\\n\\nof\\n\\nthat\\n\\nnew\\n\\na\\n\\nmajority\\n\\nmajority\\n\\n.\\n\\n<pad>\\n\\nmaking\\n\\n.\\n\\n<pad>\\n\\nor\\n\\nmore\\n\\nthis\\n\\nsince\\n\\nthe\\n\\nsince\\n\\nor\\n\\n<pad>\\n\\nspirit\\n\\n<pad>\\n\\n<EOS>\\n\\nprocess\\n\\npassed\\n\\nin\\n\\nis\\n\\nin\\n\\nprocess\\n\\nof\\n\\n2009\\n\\nregistration\\n\\nlaws\\n\\nis\\n\\na\\n\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\\n\\n13\\n\\nperfect\\n\\n just\\n\\n just\\n\\napplication\\n\\nits\\n\\nThe\\n\\njust\\n\\nLaw\\n\\nbut\\n\\nInput-Input Layer5\\n\\nshould\\n\\nbe\\n\\n,\\n\\nwill'),\n",
       " Document(metadata={'language': 'EN', 'tags': \"['AI', 'Machine Learning', 'Transformer', 'Attention Mechanism']\", 'read_time': 0.61, 'summary': 'The section discusses the applications of multi-head attention in the Transformer model, specifically in encoder-decoder attention and self-attention layers.', 'source': 'uploaded_file', 'filetype': 'application/pdf', 'last_modified': '2024-10-22T21:44:24', 'author': '', 'filename': 'attention_is_all_you_need.pdf', 'word_count': 122.0, 'content_type': 'other', 'uuid': 'd9c9c238-38c3-43e4-958c-58e7f012cb74'}, page_content='3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\nIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\\n\\nThe encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vectrix_graphs import vectordb\n",
    "# Now let's query the vector database\n",
    "vectordb.similarity_search(\n",
    "    query=\"What is the attention mechanism?\",\n",
    "    k=3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "\n",
    "\n",
    "client = chromadb.HttpClient(host='localhost', port=7777)\n",
    "\n",
    "collection = client.get_collection(\"demo\")\n",
    "collection.query(\n",
    "    query_embeddings=embeddings,\n",
    "    n_results=3,\n",
    "    where={\"metadata_field\": \"uploaded_file\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking questions to the Graph\n",
    " Let's now ask questions using the LangGraph workflow\n",
    "\n",
    "### Example 1: Using closed source LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Using open-source LLMs that can be self-hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages from the src directory\n",
    "import sys\n",
    "from IPython.display import Markdown, display, Image\n",
    "sys.path.append('../src')\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from vectrix_graphs import local_slm_demo\n",
    "\n",
    "# Display the graph\n",
    "display(Image(local_slm_demo.get_graph().draw_mermaid_png()))\n",
    "\n",
    "#Ask the question\n",
    "input = [HumanMessage(content= مامصدر؟\")]\n",
    "\n",
    "\n",
    "# Run the graph\n",
    "response = await local_slm_demo.ainvoke({\"messages\": input})\n",
    "display(Markdown(f\"***Question:*** \\n {input[0].content}\\n\"))\n",
    "display(Markdown(response['messages'][-1].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages from the src directory\n",
    "import sys\n",
    "from IPython.display import Markdown, display, Image\n",
    "sys.path.append('../src')\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from vectrix_graphs import local_slm_demo\n",
    "\n",
    "#Ask the question\n",
    "input = [HumanMessage(content=\"What is the attention mechanism?\")]\n",
    "\n",
    "# Run the graph\n",
    "response = await local_slm_demo.ainvoke({\"messages\": input})\n",
    "display(Markdown(f\"***Question:*** \\n {input[0].content}\\n\"))\n",
    "display(Markdown(response['messages'][-1].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['messages'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "human = HumanMessage(content=\"What is the attention mechanism?\")\n",
    "ai = AIMessage(content=\"The attention mechanism is a technique used in neural networks to enable the model to focus on relevant parts of the input data during processing.\")\n",
    "\n",
    "messages = ChatPromptTemplate.from_messages([human, ai])\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = messages | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
